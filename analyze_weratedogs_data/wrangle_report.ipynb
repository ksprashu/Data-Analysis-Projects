{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling the WeRateDogs Twitter Archive Data\n",
    "\n",
    "## Objective\n",
    "The objective of this report is to describe the steps taken to gather, assess and clean the data which are the tweet archive of the WeRateDogs twitter account. \n",
    "\n",
    "Users can share photos of their dogs (and pets they treat as dogs) to WeRateDogs. The said account in turns rates each of these dogs and tweets about them with some commentary and a rating.\n",
    "\n",
    "## Sources\n",
    "- The tweet archive of the last 500 tweets have been provided by WeRateDogs. The tweets containing ratings only have been filtered out by Udacity and have been provided for analysis. This is referred to as the `twitter_archive` data henceforth. This data has been provided by Udacity as a csv.\n",
    "- This archive was then put through a neural network that analysed the images attached with every tweet and detected them as a truly dogs, or other animals / objects. The predictions and the prediction confidence is a part of this data. This referred to as the `image_predictions` data henceforth. This data was downloaded as a tsv via code.\n",
    "- Finally, additional information pertaining to every tweet is available via twitter and these were accessed via authenticated API calls and downloaded as JSON data into a text file. This data will be referred to as `detailed_tweets`.\n",
    "\n",
    "## Gathering\n",
    "- Retrieving of data from the csv on the filesystem and the tsv on the http url was quite straightforward. There are functions available to do this and the read_csv() function that can read a tsv is also able to read directly from a URL. \n",
    "- However, we will be downloading the TSV file first using the requests module, and then we will read it in from the local filesystem.\n",
    "- Now we can retrieve the detailed tweets from twitter using the twitter API. There is a python library called tweepy that allows us to fetch the tweets. There is an API that allows us to fetch tweets individually and also an API that allows to fetch data in bulk. We have used the bulk API in order to avoid running into rate limitations. However we then have to slice the dataset in group of 100 and then consume the API.\n",
    "- Since we now have 2 sources of data, we have to use one of the two to retrieve the detailed tweets from the API. Comparing the two datasets purely by size, we have used the larger of the datasets as the source of tweet_ids.\n",
    "- The tweets fetched from the API are written line by line into a json file and stored on the filesystem. This is subsequently read into a dataframe by using a pandas helper method.\n",
    "\n",
    "## Assessing\n",
    "- The data has been assessed both visually and programatically. \n",
    "- Firstly the data is output using random sampling so that we can give a naked eye check on the data and look for things like consistency of naming conventions, consistency of data in the dataset and so on.\n",
    "- Next, we list the attributes of the dataset using `.info()` and `.describe()` functions to determine if the datatypes of the columns make sense or they have to be converted. We make a note of such mismatches for future cleaning\n",
    "- We then look at the tidiness of data such as whether the data itself makes sense, however can it be processed programmatically? One such sample we see is that for `source` where it is listed as a URL, however the value can be extracted for better programmatic processing.\n",
    "- While assessing the data, we also see that all the data in the 3 tables are related, and hence the tables can be joined \n",
    "- We also look for data that seem incomplete or inconsistent, and can take a call whether there is something that we can do to clean this data or whether we need more information to clean it up, or have to come back later for a cleaning. Eg: ratings are something that we could take a look at and cleanup for a majority of cases. However dog state categorization such as doggo, floofer, puppo is not something that we can do something about since we don't have access to the image prediction model.\n",
    "- The dog state is also some thing that can be tidied up and melted into a single column, but since the data is not complete, this is something we decided to not take up at this point\n",
    "- There are a lot of such quality and tidiness attributes that can be cleaned up, and we decided to do multiple assess+clean iterations so that we could come back to a cleaner dataset before doing further assessments. \n",
    "\n",
    "## Cleaning\n",
    "- We clean two broad categories of issues - quality and tidiness.\n",
    "- For quality, we start with cleaning datatype issues and converting the columns to the correct data types\n",
    "- For the rows with missing or invalid data that we don't use, we filter out those rows and reassign bacck to the dataframe for further use\n",
    "- Then the columns that we don't want to use are dropped\n",
    "- This is repeated for each of the 3 dataframes that we have captured\n",
    "- Most of the cases in this dataset, we don't have the ability to fix bad data (such as dog state) and we just leave the data as it is for another time.\n",
    "- For the tidiness cleanup, again we go through the points that we have assessed and make the necessary corrections. We are able to use the default pandas functionality to make corrections to the data, such as title() function of the string helper, and similarly extract(), replace() functions.\n",
    "- Finally, we use the merge() function to join the 3 tables together. In order to get at the best quality of data, we use inner join to keep only the data where we have all the attributes that we are interested in. \n",
    "\n",
    "## Analysis\n",
    "- We have a fair understanding of the data thanks to the assessment and cleaning phases. Now we can pose questions about the data that we want to get insights into.\n",
    "- Using standard pandas and matplotlib functionality, we can apply filters and extract the data that we are intersted in and observe the values. \n",
    "- We are also able to plot data in a visual format for better understanding and interpretations of the data that might not be possible from a simple programmatic approach. \n",
    "- Some common visualtions that we have used are relation plots (as scatter and line plots), and histograms (for ratings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
